{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea2fe32-6c33-41e5-89e5-1001532f3b83",
   "metadata": {},
   "source": [
    "# Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742c7d2-db17-4759-836f-fd3405010490",
   "metadata": {},
   "source": [
    "To solve this problem, we need to use Bayes' theorem, which relates conditional probabilities. Let's define:\n",
    "\n",
    "A: an employee uses the company's health insurance plan\n",
    "B: an employee is a smoker\n",
    "\n",
    "We want to find the probability of an employee being a smoker given that he/she uses the health insurance plan, which is P(B|A).\n",
    "\n",
    "We know that 70% of the employees use the health insurance plan, which means P(A) = 0.7.\n",
    "\n",
    "We also know that 40% of the employees who use the plan are smokers, which means P(B|A) = 0.4.\n",
    "\n",
    "Bayes' theorem states that: P(B|A) = P(A|B) * P(B) / P(A)\n",
    "\n",
    "We need to find P(B), which is the probability of an employee being a smoker regardless of whether they use the health insurance plan or not. We can use the law of total probability to calculate it:\n",
    "\n",
    "P(B) = P(B|A) * P(A) + P(B|A') * P(A')\n",
    "\n",
    "where A' means an employee does not use the health insurance plan. We can assume that the percentage of non-users of the plan who are smokers is negligible, so P(B|A') ≈ 0. Therefore:\n",
    "\n",
    "P(B) ≈ P(B|A) * P(A) + 0\n",
    "\n",
    "P(B) ≈ 0.4 * 0.7 = 0.28\n",
    "\n",
    "Now we can plug in all the values into Bayes' theorem:\n",
    "\n",
    "P(B|A) = P(A|B) * P(B) / P(A)\n",
    "\n",
    "P(B|A) = P(A and B) / P(A)\n",
    "\n",
    "P(B|A) = P(B|A) * P(A) / P(A)\n",
    "\n",
    "P(B|A) = 0.4 * 0.7 / 0.7\n",
    "\n",
    "P(B|A) = 0.4\n",
    "\n",
    "Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.4 or 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2d5277-ee02-41f0-a0ec-d7b676c5db2d",
   "metadata": {},
   "source": [
    "# Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44e6fa4-e415-4aca-9576-45dea3c51d07",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are both variants of the Naive Bayes algorithm, which is a popular algorithm for classification tasks in machine learning. While they are both based on the same underlying principles, there are some differences in the way they handle data.\n",
    "\n",
    "Bernoulli Naive Bayes is typically used when the features are binary & it takes only two values, 0 & 1. It is commonly used in text classification tasks, where each feature represents the presence or absence of a particular word in a document. In Bernoulli Naive Bayes, each feature is modeled as a binary random variable, with the assumption that each feature is conditionally independent given the class. This means that the presence or absence of one feature does not affect the probability of the presence or absence of any other feature. The algorithm then calculates the conditional probability of each class given the presence or absence of each feature, using Bayes' theorem.\n",
    "\n",
    "Multinomial Naive Bayes, on the other hand, is used when the features are discrete & it takes some non-negative integer values. It is commonly used in text classification tasks, where each feature represents the count of a particular word in a document. In Multinomial Naive Bayes, each feature is modeled as a multinomial random variable, with the assumption that each feature is conditionally independent given the class. This means that the count of one feature does not affect the probability of the count of any other feature. The algorithm then calculates the conditional probability of each class given the count of each feature, using Bayes' theorem.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes is used for binary features, while Multinomial Naive Bayes is used for discrete count features. Both algorithms assume that each feature is conditionally independent given the class, and both calculate the conditional probability of each class given the features using Bayes' theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8399a036-bcc8-40cc-b16b-86305ffe22aa",
   "metadata": {},
   "source": [
    "# Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be35734-8ead-4703-bf65-e102de7a7c42",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes is a classification algorithm that is commonly used in natural language processing tasks such as text classification. It is a variant of the Naive Bayes algorithm that assumes that the features are binary or Boolean, indicating whether a particular feature is present or not.\n",
    "\n",
    "In the case of missing values in the input data, Bernoulli Naive Bayes handles them by simply ignoring the missing values and treating them as if they were not present in the data. This is because the algorithm assumes that the features are independent of each other, and therefore the absence of a particular feature does not affect the probability of the presence of another feature.\n",
    "\n",
    "However, it is important to note that the presence or absence of certain features can have a significant impact on the classification accuracy of the algorithm. Therefore, it is recommended to handle missing values in the input data by imputing correct values, such as the mean or median value of that desired feature before applying the Bernoulli Naive Bayes algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aec738a-93f4-443f-94c4-a44a7ed32424",
   "metadata": {},
   "source": [
    "# Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c017320-7f29-47b6-9d8b-a13d70d44403",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. The algorithm can be extended to handle multiple classes by using the \"one-vs-all\" or \"one-vs-rest\" strategy, where the algorithm trains multiple binary classifiers, one for each class, and then combines their results to make the final prediction.\n",
    "\n",
    "In the \"one-vs-all\" strategy, for each class, the algorithm considers all instances of that class as positive, as well as, negative examples. It then trains a binary classifier for each class using the Gaussian Naive Bayes algorithm. During prediction, the algorithm applies each classifier to the input instance and selects the class with the highest probability as the final prediction.\n",
    "\n",
    "Alternatively, in the \"one-vs-rest\" strategy, the algorithm considers each class separately and treats it as the positive, as well as, negative class. It then trains a binary classifier for each class using the Gaussian Naive Bayes algorithm. During prediction, the algorithm applies each classifier to the input instance and selects the class with the highest probability as the final prediction.\n",
    "\n",
    "Overall, Gaussian Naive Bayes is a powerful and efficient algorithm for multi-class classification tasks, especially in situations where the feature variables are continuous and have a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5df0a5e-195b-4a35-ae66-b1f177706e02",
   "metadata": {},
   "source": [
    "# Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf22e871-c3b7-4672-ab8d-274973c64aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes:\n",
      "Accuracy: 0.88\n",
      "Precision: 0.89\n",
      "Recall: 0.82\n",
      "F1 Score: 0.85\n",
      "\n",
      "Multinomial Naive Bayes:\n",
      "Accuracy: 0.79\n",
      "Precision: 0.74\n",
      "Recall: 0.72\n",
      "F1 Score: 0.73\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "Accuracy: 0.82\n",
      "Precision: 0.71\n",
      "Recall: 0.96\n",
      "F1 Score: 0.81\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data', header=None)\n",
    "X = data.iloc[:, :-1].values\n",
    "y = data.iloc[:, -1].values\n",
    "\n",
    "# Bernoulli Naive Bayes classifier\n",
    "bnb = BernoulliNB()\n",
    "bnb_scores = cross_val_score(bnb, X, y, cv=10)\n",
    "\n",
    "# Multinomial Naive Bayes classifier\n",
    "mnb = MultinomialNB()\n",
    "mnb_scores = cross_val_score(mnb, X, y, cv=10)\n",
    "\n",
    "# Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "gnb_scores = cross_val_score(gnb, X, y, cv=10)\n",
    "\n",
    "# Print the results\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print(\"Accuracy: {:.2f}\".format(bnb_scores.mean()))\n",
    "print(\"Precision: {:.2f}\".format(cross_val_score(bnb, X, y, cv=10, scoring='precision').mean()))\n",
    "print(\"Recall: {:.2f}\".format(cross_val_score(bnb, X, y, cv=10, scoring='recall').mean()))\n",
    "print(\"F1 Score: {:.2f}\".format(cross_val_score(bnb, X, y, cv=10, scoring='f1').mean()))\n",
    "\n",
    "print(\"\\nMultinomial Naive Bayes:\")\n",
    "print(\"Accuracy: {:.2f}\".format(mnb_scores.mean()))\n",
    "print(\"Precision: {:.2f}\".format(cross_val_score(mnb, X, y, cv=10, scoring='precision').mean()))\n",
    "print(\"Recall: {:.2f}\".format(cross_val_score(mnb, X, y, cv=10, scoring='recall').mean()))\n",
    "print(\"F1 Score: {:.2f}\".format(cross_val_score(mnb, X, y, cv=10, scoring='f1').mean()))\n",
    "\n",
    "print(\"\\nGaussian Naive Bayes:\")\n",
    "print(\"Accuracy: {:.2f}\".format(gnb_scores.mean()))\n",
    "print(\"Precision: {:.2f}\".format(cross_val_score(gnb, X, y, cv=10, scoring='precision').mean()))\n",
    "print(\"Recall: {:.2f}\".format(cross_val_score(gnb, X, y, cv=10, scoring='recall').mean()))\n",
    "print(\"F1 Score: {:.2f}\".format(cross_val_score(gnb, X, y, cv=10, scoring='f1').mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc419db1-6de3-4681-b90f-27a3422606af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
